{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用CNN 进行训练 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "# print(1)\n",
    "\n",
    "# import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   image_name  width  height  xmin  ymin  xmax  ymax\n",
      "0    0001.jpg   1956    1934   631   903   839  1121\n",
      "1    0002.jpg   1956    1934   653   901   883  1131\n",
      "2    0003.jpg   2992    2000  1368   939  1570  1137\n",
      "3    0004.jpg   2992    2000  1560  1006  1757  1202\n",
      "4    0005.jpg   2992    2000  1637  1016  1826  1202\n",
      "..        ...    ...     ...   ...   ...   ...   ...\n",
      "75   0076.jpg   2992    2000  1407   933  1582  1106\n",
      "76   0077.jpg   2992    2000  1203   837  1399  1037\n",
      "77   0078.jpg   2992    2000  1570  1006  1766  1199\n",
      "78   0079.jpg   2992    2000  1187  1037  1355  1222\n",
      "79   0080.jpg   2992    2000  1364   943  1549  1118\n",
      "\n",
      "[80 rows x 7 columns]\n",
      "   image_name  width  height      xmin      ymin      xmax      ymax\n",
      "0    0001.jpg   1956    1934  0.322597  0.466908  0.428937  0.579628\n",
      "1    0002.jpg   1956    1934  0.333845  0.465874  0.451431  0.584798\n",
      "2    0003.jpg   2992    2000  0.457219  0.469500  0.524733  0.568500\n",
      "3    0004.jpg   2992    2000  0.521390  0.503000  0.587233  0.601000\n",
      "4    0005.jpg   2992    2000  0.547126  0.508000  0.610294  0.601000\n",
      "..        ...    ...     ...       ...       ...       ...       ...\n",
      "75   0076.jpg   2992    2000  0.470254  0.466500  0.528743  0.553000\n",
      "76   0077.jpg   2992    2000  0.402072  0.418500  0.467580  0.518500\n",
      "77   0078.jpg   2992    2000  0.524733  0.503000  0.590241  0.599500\n",
      "78   0079.jpg   2992    2000  0.396725  0.518500  0.452874  0.611000\n",
      "79   0080.jpg   2992    2000  0.455882  0.471500  0.517714  0.559000\n",
      "\n",
      "[80 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import uniform\n",
    "import csv\n",
    "from skimage.feature import hog\n",
    "import os\n",
    "import xmltodict\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "def parse_xml_to_dataframe(xml_folder):\n",
    "    data = []\n",
    "    \n",
    "    # 遍历 XML 文件夹中的每个文件\n",
    "    for xml_file in os.listdir(xml_folder):\n",
    "        if xml_file.endswith('.xml'):\n",
    "            with open(os.path.join(xml_folder, xml_file), 'r') as file:\n",
    "                xml_data = xmltodict.parse(file.read())\n",
    "            \n",
    "            # 提取关键信息\n",
    "            annotation = xml_data['annotation']\n",
    "            filename = annotation['filename']\n",
    "            size = annotation['size']\n",
    "            objects = annotation['object']\n",
    "            \n",
    "            # 提取图像尺寸\n",
    "            width = int(size['width'])\n",
    "            height = int(size['height'])\n",
    "            \n",
    "            # 如果存在多个对象，需要处理成列表\n",
    "            if isinstance(objects, list):\n",
    "                for obj in objects:\n",
    "                    bndbox = obj['bndbox']\n",
    "                    data.append({\n",
    "                        'image_name': filename,\n",
    "                        'width': width,\n",
    "                        'height': height,\n",
    "                        'xmin': int(bndbox['xmin']),\n",
    "                        'ymin': int(bndbox['ymin']),\n",
    "                        'xmax': int(bndbox['xmax']),\n",
    "                        'ymax': int(bndbox['ymax'])\n",
    "                    })\n",
    "            else:\n",
    "                bndbox = objects['bndbox']\n",
    "                data.append({\n",
    "                    'image_name': filename,\n",
    "                    'width': width,\n",
    "                    'height': height,\n",
    "                    'xmin': int(bndbox['xmin']),\n",
    "                    'ymin': int(bndbox['ymin']),\n",
    "                    'xmax': int(bndbox['xmax']),\n",
    "                    'ymax': int(bndbox['ymax'])\n",
    "                })\n",
    "\n",
    "    # 转换为 Pandas DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "# 调用函数解析 XML 数据\n",
    "xml_folder = 'C:/code/vcpython/ML_design_1/task1/detection/train_location'  # 替换为实际路径\n",
    "train_location_df = parse_xml_to_dataframe(xml_folder)\n",
    "print(train_location_df)\n",
    "\n",
    "\n",
    "def normalize_coordinates(df):\n",
    "    df['xmin'] = df['xmin'] / df['width']\n",
    "    df['ymin'] = df['ymin'] / df['height']\n",
    "    df['xmax'] = df['xmax'] / df['width']\n",
    "    df['ymax'] = df['ymax'] / df['height']\n",
    "    return df\n",
    "\n",
    "# 对边框数据归一化\n",
    "train_location_df = normalize_coordinates(train_location_df)\n",
    "print(train_location_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用SVM回归预测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、导入包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import uniform\n",
    "import csv\n",
    "from skimage.feature import hog\n",
    "import os\n",
    "import xmltodict\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2、读取数据,处理xml文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   image_name  width  height  xmin  ymin  xmax  ymax\n",
      "0    0001.jpg   1956    1934   631   903   839  1121\n",
      "1    0002.jpg   1956    1934   653   901   883  1131\n",
      "2    0003.jpg   2992    2000  1368   939  1570  1137\n",
      "3    0004.jpg   2992    2000  1560  1006  1757  1202\n",
      "4    0005.jpg   2992    2000  1637  1016  1826  1202\n",
      "..        ...    ...     ...   ...   ...   ...   ...\n",
      "75   0076.jpg   2992    2000  1407   933  1582  1106\n",
      "76   0077.jpg   2992    2000  1203   837  1399  1037\n",
      "77   0078.jpg   2992    2000  1570  1006  1766  1199\n",
      "78   0079.jpg   2992    2000  1187  1037  1355  1222\n",
      "79   0080.jpg   2992    2000  1364   943  1549  1118\n",
      "\n",
      "[80 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "def parse_xml_to_dataframe(xml_folder):\n",
    "    data = []\n",
    "    \n",
    "    # 遍历 XML 文件夹中的每个文件\n",
    "    for xml_file in os.listdir(xml_folder):\n",
    "        if xml_file.endswith('.xml'):\n",
    "            with open(os.path.join(xml_folder, xml_file), 'r') as file:\n",
    "                xml_data = xmltodict.parse(file.read())\n",
    "            \n",
    "            # 提取关键信息\n",
    "            annotation = xml_data['annotation']\n",
    "            filename = annotation['filename']\n",
    "            size = annotation['size']\n",
    "            objects = annotation['object']\n",
    "            \n",
    "            # 提取图像尺寸\n",
    "            width = int(size['width'])\n",
    "            height = int(size['height'])\n",
    "            \n",
    "            # 如果存在多个对象，需要处理成列表\n",
    "            if isinstance(objects, list):\n",
    "                for obj in objects:\n",
    "                    bndbox = obj['bndbox']\n",
    "                    data.append({\n",
    "                        'image_name': filename,\n",
    "                        'width': width,\n",
    "                        'height': height,\n",
    "                        'xmin': int(bndbox['xmin']),\n",
    "                        'ymin': int(bndbox['ymin']),\n",
    "                        'xmax': int(bndbox['xmax']),\n",
    "                        'ymax': int(bndbox['ymax'])\n",
    "                    })\n",
    "            else:\n",
    "                bndbox = objects['bndbox']\n",
    "                data.append({\n",
    "                    'image_name': filename,\n",
    "                    'width': width,\n",
    "                    'height': height,\n",
    "                    'xmin': int(bndbox['xmin']),\n",
    "                    'ymin': int(bndbox['ymin']),\n",
    "                    'xmax': int(bndbox['xmax']),\n",
    "                    'ymax': int(bndbox['ymax'])\n",
    "                })\n",
    "\n",
    "    # 转换为 Pandas DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "# 调用函数解析 XML 数据\n",
    "xml_folder = 'C:/code/vcpython/ML_design_1/task1/detection/train_location'  # 替换为实际路径\n",
    "train_location_df = parse_xml_to_dataframe(xml_folder)\n",
    "print(train_location_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3、边框数据归一化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   image_name  width  height      xmin      ymin      xmax      ymax\n",
      "0    0001.jpg   1956    1934  0.322597  0.466908  0.428937  0.579628\n",
      "1    0002.jpg   1956    1934  0.333845  0.465874  0.451431  0.584798\n",
      "2    0003.jpg   2992    2000  0.457219  0.469500  0.524733  0.568500\n",
      "3    0004.jpg   2992    2000  0.521390  0.503000  0.587233  0.601000\n",
      "4    0005.jpg   2992    2000  0.547126  0.508000  0.610294  0.601000\n",
      "..        ...    ...     ...       ...       ...       ...       ...\n",
      "75   0076.jpg   2992    2000  0.470254  0.466500  0.528743  0.553000\n",
      "76   0077.jpg   2992    2000  0.402072  0.418500  0.467580  0.518500\n",
      "77   0078.jpg   2992    2000  0.524733  0.503000  0.590241  0.599500\n",
      "78   0079.jpg   2992    2000  0.396725  0.518500  0.452874  0.611000\n",
      "79   0080.jpg   2992    2000  0.455882  0.471500  0.517714  0.559000\n",
      "\n",
      "[80 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "def normalize_coordinates(df):\n",
    "    df['xmin'] = df['xmin'] / df['width']\n",
    "    df['ymin'] = df['ymin'] / df['height']\n",
    "    df['xmax'] = df['xmax'] / df['width']\n",
    "    df['ymax'] = df['ymax'] / df['height']\n",
    "    return df\n",
    "\n",
    "# 对边框数据归一化\n",
    "train_location_df = normalize_coordinates(train_location_df)\n",
    "print(train_location_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4、整合信息，将csv文件整合到边框中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#格式化fovea_localization_train_GT.csv文件\n",
    "fovea_gt_copy = pd.read_csv('C:/code/vcpython/ML_design_1/task1/detection/fovea_localization_train_GT_copy.csv')\n",
    "\n",
    "def format_fovea_gt(df):\n",
    "    for i in df['image_name']:\n",
    "        \n",
    "        df['image_name'][i - 1] = f'{i:04d}.jpg'\n",
    "        # print(i)\n",
    "    return df\n",
    "\n",
    "fovea_gt_copy = format_fovea_gt(fovea_gt_copy)\n",
    "# print(fovea_gt_copy)\n",
    "\n",
    "fovea_gt_copy.to_csv('C:/code/vcpython/ML_design_1/task1/detection/fovea_localization_train_GT_copy.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5、对黄斑区域数据进行预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.0026855341043829383\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['svm_regressor_ymax.pkl']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from skimage.feature import hog\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "# 提取图像的 HOG 特征\n",
    "def extract_hog_features(image_path):\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)  # 转为灰度图\n",
    "    img = cv2.resize(img, (256, 256))  # 调整图像大小\n",
    "    features, _ = hog(img, orientations=9, pixels_per_cell=(8, 8), cells_per_block=(2, 2), visualize=True)\n",
    "    return features\n",
    "\n",
    "# 准备数据\n",
    "def prepare_data(df, image_folder):\n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        # 获取图像路径和黄斑区域坐标\n",
    "        image_path = os.path.join(image_folder, row['image_name'])\n",
    "        xmin, ymin, xmax, ymax = row['xmin'], row['ymin'], row['xmax'], row['ymax']\n",
    "        \n",
    "        # 提取 HOG 特征\n",
    "        feature = extract_hog_features(image_path)\n",
    "        \n",
    "        # 将归一化坐标作为目标值\n",
    "        labels.append([xmin, ymin, xmax, ymax])\n",
    "        features.append(feature)\n",
    "    \n",
    "    return np.array(features), np.array(labels)\n",
    "\n",
    "# 加载数据\n",
    "image_folder = 'C:/code/vcpython/ML_design_1/task1/detection/train'  # 图像文件夹路径\n",
    "X, y = prepare_data(train_location_df, image_folder)\n",
    "\n",
    "# 切分训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 使用标准化管道\n",
    "svm_regressor_xmin = make_pipeline(StandardScaler(), SVR(kernel='rbf', C=1, gamma=0.01))\n",
    "svm_regressor_ymin = make_pipeline(StandardScaler(), SVR(kernel='rbf', C=1, gamma=0.01))\n",
    "svm_regressor_xmax = make_pipeline(StandardScaler(), SVR(kernel='rbf', C=1, gamma=0.01))\n",
    "svm_regressor_ymax = make_pipeline(StandardScaler(), SVR(kernel='rbf', C=1, gamma=0.01))\n",
    "\n",
    "# 训练模型\n",
    "svm_regressor_xmin.fit(X_train, y_train[:, 0])\n",
    "svm_regressor_ymin.fit(X_train, y_train[:, 1])\n",
    "svm_regressor_xmax.fit(X_train, y_train[:, 2])\n",
    "svm_regressor_ymax.fit(X_train, y_train[:, 3])\n",
    "\n",
    "# 预测\n",
    "y_pred_xmin = svm_regressor_xmin.predict(X_test)\n",
    "y_pred_ymin = svm_regressor_ymin.predict(X_test)\n",
    "y_pred_xmax = svm_regressor_xmax.predict(X_test)\n",
    "y_pred_ymax = svm_regressor_ymax.predict(X_test)\n",
    "\n",
    "# 合并预测结果\n",
    "y_pred = np.stack((y_pred_xmin, y_pred_ymin, y_pred_xmax, y_pred_ymax), axis=1)\n",
    "\n",
    "# 计算误差\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "\n",
    "# 你也可以保存模型以便后续使用\n",
    "import joblib\n",
    "joblib.dump(svm_regressor_xmin, 'svm_regressor_xmin.pkl')\n",
    "joblib.dump(svm_regressor_ymin, 'svm_regressor_ymin.pkl')\n",
    "joblib.dump(svm_regressor_xmax, 'svm_regressor_xmax.pkl')\n",
    "joblib.dump(svm_regressor_ymax, 'svm_regressor_ymax.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6、绘制黄斑框"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 8100 features, but StandardScaler is expecting 34596 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 49\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# 预测\u001b[39;00m\n\u001b[0;32m     48\u001b[0m test_features \u001b[38;5;241m=\u001b[39m test_features\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# 确保是 2D 输入\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m predicted_xmin \u001b[38;5;241m=\u001b[39m \u001b[43msvm_regressor_xmin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m predicted_ymin \u001b[38;5;241m=\u001b[39m svm_regressor_ymin\u001b[38;5;241m.\u001b[39mpredict(test_features)\n\u001b[0;32m     51\u001b[0m predicted_xmax \u001b[38;5;241m=\u001b[39m svm_regressor_xmax\u001b[38;5;241m.\u001b[39mpredict(test_features)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\pipeline.py:514\u001b[0m, in \u001b[0;36mPipeline.predict\u001b[1;34m(self, X, **predict_params)\u001b[0m\n\u001b[0;32m    512\u001b[0m Xt \u001b[38;5;241m=\u001b[39m X\n\u001b[0;32m    513\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, name, transform \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter(with_final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m--> 514\u001b[0m     Xt \u001b[38;5;241m=\u001b[39m \u001b[43mtransform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mpredict(Xt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpredict_params)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_set_output.py:157\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 157\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    159\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    160\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    161\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    162\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    163\u001b[0m         )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\preprocessing\\_data.py:1006\u001b[0m, in \u001b[0;36mStandardScaler.transform\u001b[1;34m(self, X, copy)\u001b[0m\n\u001b[0;32m   1003\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m   1005\u001b[0m copy \u001b[38;5;241m=\u001b[39m copy \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy\n\u001b[1;32m-> 1006\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1007\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1008\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1009\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1010\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1011\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1012\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1013\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1015\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparse\u001b[38;5;241m.\u001b[39missparse(X):\n\u001b[0;32m   1016\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwith_mean:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:626\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    623\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m--> 626\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_n_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:415\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    412\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_features \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_:\n\u001b[1;32m--> 415\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    416\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    417\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    418\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: X has 8100 features, but StandardScaler is expecting 34596 features as input."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import joblib\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def draw_bounding_box(image_path, bbox, output_path=None):\n",
    "    \"\"\"\n",
    "    在图像上绘制预测框\n",
    "    :param image_path: 图像路径\n",
    "    :param bbox: 预测框，格式为 [xmin, ymin, xmax, ymax]，归一化坐标\n",
    "    :param output_path: 保存绘制结果的路径\n",
    "    \"\"\"\n",
    "    img = cv2.imread(image_path)\n",
    "    height, width = img.shape[:2]\n",
    "\n",
    "    # 反归一化坐标\n",
    "    xmin = int(bbox[0] * width)\n",
    "    ymin = int(bbox[1] * height)\n",
    "    xmax = int(bbox[2] * width)\n",
    "    ymax = int(bbox[3] * height)\n",
    "\n",
    "    # 绘制矩形框\n",
    "    cv2.rectangle(img, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n",
    "\n",
    "    # 保存结果或显示\n",
    "    if output_path:\n",
    "        cv2.imwrite(output_path, img)\n",
    "    else:\n",
    "        cv2.imshow(\"Prediction\", img)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "# 加载模型\n",
    "svm_regressor_xmin = joblib.load('svm_regressor_xmin.pkl')\n",
    "svm_regressor_ymin = joblib.load('svm_regressor_ymin.pkl')\n",
    "svm_regressor_xmax = joblib.load('svm_regressor_xmax.pkl')\n",
    "svm_regressor_ymax = joblib.load('svm_regressor_ymax.pkl')\n",
    "\n",
    "for i in range(81, 101):\n",
    "    \n",
    "  # 加载测试图像和提取特征\n",
    "  test_image_path = 'C:/code/vcpython/ML_design_1/task1/detection/test/' + f'{i:04d}.jpg'\n",
    "  test_image = cv2.imread(test_image_path, cv2.IMREAD_GRAYSCALE)\n",
    "  test_image = cv2.resize(test_image, (128, 128))\n",
    "  test_features, _ = hog(test_image, orientations=9, pixels_per_cell=(8, 8), cells_per_block=(2, 2), visualize=True)\n",
    "\n",
    "  # 预测\n",
    "  test_features = test_features.reshape(1, -1)  # 确保是 2D 输入\n",
    "  predicted_xmin = svm_regressor_xmin.predict(test_features)\n",
    "  predicted_ymin = svm_regressor_ymin.predict(test_features)\n",
    "  predicted_xmax = svm_regressor_xmax.predict(test_features)\n",
    "  predicted_ymax = svm_regressor_ymax.predict(test_features)\n",
    "\n",
    "  predicted_bbox = [predicted_xmin[0], predicted_ymin[0], predicted_xmax[0], predicted_ymax[0]]\n",
    "\n",
    "  # 绘制预测框\n",
    "  draw_bounding_box(test_image_path, predicted_bbox)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   image_name  width  height      xmin      ymin      xmax      ymax  \\\n",
      "0    0001.jpg   1956    1934  0.322597  0.466908  0.428937  0.579628   \n",
      "1    0002.jpg   1956    1934  0.333845  0.465874  0.451431  0.584798   \n",
      "2    0003.jpg   2992    2000  0.457219  0.469500  0.524733  0.568500   \n",
      "3    0004.jpg   2992    2000  0.521390  0.503000  0.587233  0.601000   \n",
      "4    0005.jpg   2992    2000  0.547126  0.508000  0.610294  0.601000   \n",
      "..        ...    ...     ...       ...       ...       ...       ...   \n",
      "75   0076.jpg   2992    2000  0.470254  0.466500  0.528743  0.553000   \n",
      "76   0077.jpg   2992    2000  0.402072  0.418500  0.467580  0.518500   \n",
      "77   0078.jpg   2992    2000  0.524733  0.503000  0.590241  0.599500   \n",
      "78   0079.jpg   2992    2000  0.396725  0.518500  0.452874  0.611000   \n",
      "79   0080.jpg   2992    2000  0.455882  0.471500  0.517714  0.559000   \n",
      "\n",
      "        Fovea_X      Fovea_Y  \n",
      "0    734.031331  1016.817066  \n",
      "1    769.024859  1018.059762  \n",
      "2   1470.287475  1037.601074  \n",
      "3   1658.316671  1104.020062  \n",
      "4   1729.828431  1107.209978  \n",
      "..          ...          ...  \n",
      "75  1494.938977  1017.492746  \n",
      "76  1299.490121   934.244593  \n",
      "77  1665.401957  1101.482670  \n",
      "78  1270.251821  1134.462757  \n",
      "79  1457.479958  1033.677183  \n",
      "\n",
      "[80 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "# 加载中心凹坐标\n",
    "fovea_gt = pd.read_csv('C:/code/vcpython/ML_design_1/task1/detection/fovea_localization_train_GT_copy.csv')\n",
    "# print(fovea_gt.columns)\n",
    "\n",
    "# 确保列名一致\n",
    "# fovea_gt.rename(columns={'image_name': 'image_name'}, inplace=True)\n",
    "\n",
    "# 归一化中心凹坐标\n",
    "# fovea_gt['Fovea_X'] = fovea_gt['Fovea_X'] / train_location_df['width']  # 假设宽度和图像相同\n",
    "# fovea_gt['Fovea_Y'] = fovea_gt['Fovea_Y'] / train_location_df['height']\n",
    "\n",
    "# print(fovea_gt)\n",
    "\n",
    "# 合并边框数据和中心凹坐标\n",
    "train_data = pd.merge(train_location_df, fovea_gt, on='image_name')\n",
    "print(train_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5、存储清洗过后的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv('cleaned_train_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6、使用SVM进行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Accuracy: 0.46875\n",
      "Mean Squared Error for fovea_x: 23935.13774945755\n",
      "Mean Squared Error for fovea_y: 1909.9304253350995\n",
      "Predicted fovea coordinates: (1469.5253168020552, 1047.6424107564362)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenovo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but SVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\lenovo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but SVR was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\lenovo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but SVR was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "\n",
    "# 读取数据\n",
    "data = pd.read_csv('cleaned_train_data.csv', header=None)\n",
    "data.columns = ['image_name', 'width', 'height', 'x1', 'y1', 'x2', 'y2', 'fovea_x', 'fovea_y']\n",
    "\n",
    "# 添加一个标签列，假设中心凹的框是由x1, y1, x2, y2定义的\n",
    "data['label'] = 1  # 假设所有数据都是中心凹的框\n",
    "\n",
    "# 特征和目标变量\n",
    "X = data[['width', 'height', 'x1', 'y1', 'x2', 'y2']]\n",
    "y_class = data['label']\n",
    "y_reg = data[['fovea_x', 'fovea_y']]\n",
    "\n",
    "# 分割数据集\n",
    "X_train, X_test, y_class_train, y_class_test, y_reg_train, y_reg_test = train_test_split(X, y_class, y_reg, test_size=0.2, random_state=42)\n",
    "\n",
    "# 训练SVM分类模型\n",
    "svc = SVC(kernel='rbf')\n",
    "svc.fit(X_train, y_class_train)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
