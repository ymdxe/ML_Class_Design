{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import pandas as pd \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unet训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Train Loss: 0.8387, Val Loss: 0.9508\n",
      "Epoch [2/100], Train Loss: 0.6082, Val Loss: 0.6489\n",
      "Epoch [3/100], Train Loss: 0.5293, Val Loss: 0.4973\n",
      "Epoch [4/100], Train Loss: 0.4850, Val Loss: 0.4938\n",
      "Epoch [5/100], Train Loss: 0.4405, Val Loss: 0.4402\n",
      "Epoch [6/100], Train Loss: 0.4196, Val Loss: 0.4623\n",
      "Epoch [7/100], Train Loss: 0.3962, Val Loss: 0.3860\n",
      "Epoch [8/100], Train Loss: 0.3858, Val Loss: 0.4452\n",
      "Epoch [9/100], Train Loss: 0.3641, Val Loss: 0.3682\n",
      "Epoch [10/100], Train Loss: 0.3560, Val Loss: 0.6349\n",
      "Epoch [11/100], Train Loss: 0.3459, Val Loss: 0.3396\n",
      "Epoch [12/100], Train Loss: 0.3323, Val Loss: 0.3315\n",
      "Epoch [13/100], Train Loss: 0.3271, Val Loss: 0.3168\n",
      "Epoch [14/100], Train Loss: 0.3156, Val Loss: 0.3208\n",
      "Epoch [15/100], Train Loss: 0.3102, Val Loss: 0.3244\n",
      "Epoch [16/100], Train Loss: 0.2949, Val Loss: 0.3129\n",
      "Epoch [17/100], Train Loss: 0.2909, Val Loss: 0.3020\n",
      "Epoch [18/100], Train Loss: 0.2813, Val Loss: 0.2845\n",
      "Epoch [19/100], Train Loss: 0.2741, Val Loss: 0.2804\n",
      "Epoch [20/100], Train Loss: 0.2666, Val Loss: 0.3203\n",
      "Epoch [21/100], Train Loss: 0.2595, Val Loss: 0.2608\n",
      "Epoch [22/100], Train Loss: 0.2527, Val Loss: 0.2566\n",
      "Epoch [23/100], Train Loss: 0.2453, Val Loss: 0.2466\n",
      "Epoch [24/100], Train Loss: 0.2382, Val Loss: 0.2406\n",
      "Epoch [25/100], Train Loss: 0.2285, Val Loss: 0.2292\n",
      "Epoch [26/100], Train Loss: 0.2208, Val Loss: 0.2131\n",
      "Epoch [27/100], Train Loss: 0.2149, Val Loss: 0.2141\n",
      "Epoch [28/100], Train Loss: 0.2088, Val Loss: 0.2058\n",
      "Epoch [29/100], Train Loss: 0.2041, Val Loss: 0.2027\n",
      "Epoch [30/100], Train Loss: 0.1985, Val Loss: 0.1956\n",
      "Epoch [31/100], Train Loss: 0.1939, Val Loss: 0.1907\n",
      "Epoch [32/100], Train Loss: 0.1889, Val Loss: 0.1888\n",
      "Epoch [33/100], Train Loss: 0.1850, Val Loss: 0.1841\n",
      "Epoch [34/100], Train Loss: 0.1798, Val Loss: 0.1765\n",
      "Epoch [35/100], Train Loss: 0.1756, Val Loss: 0.1747\n",
      "Epoch [36/100], Train Loss: 0.1714, Val Loss: 0.1708\n",
      "Epoch [37/100], Train Loss: 0.1673, Val Loss: 0.1661\n",
      "Epoch [38/100], Train Loss: 0.1631, Val Loss: 0.1628\n",
      "Epoch [39/100], Train Loss: 0.1587, Val Loss: 0.1580\n",
      "Epoch [40/100], Train Loss: 0.1550, Val Loss: 0.1558\n",
      "Epoch [41/100], Train Loss: 0.1511, Val Loss: 0.1509\n",
      "Epoch [42/100], Train Loss: 0.1471, Val Loss: 0.1479\n",
      "Epoch [43/100], Train Loss: 0.1437, Val Loss: 0.1450\n",
      "Epoch [44/100], Train Loss: 0.1403, Val Loss: 0.1426\n",
      "Epoch [45/100], Train Loss: 0.1373, Val Loss: 0.1379\n",
      "Epoch [46/100], Train Loss: 0.1343, Val Loss: 0.1363\n",
      "Epoch [47/100], Train Loss: 0.1310, Val Loss: 0.1331\n",
      "Epoch [48/100], Train Loss: 0.1278, Val Loss: 0.1279\n",
      "Epoch [49/100], Train Loss: 0.1248, Val Loss: 0.1244\n",
      "Epoch [50/100], Train Loss: 0.1220, Val Loss: 0.1243\n",
      "Epoch [51/100], Train Loss: 0.1189, Val Loss: 0.1226\n",
      "Epoch [52/100], Train Loss: 0.1160, Val Loss: 0.1183\n",
      "Epoch [53/100], Train Loss: 0.1136, Val Loss: 0.1131\n",
      "Epoch [54/100], Train Loss: 0.1109, Val Loss: 0.1139\n",
      "Epoch [55/100], Train Loss: 0.1084, Val Loss: 0.1099\n",
      "Epoch [56/100], Train Loss: 0.1065, Val Loss: 0.1081\n",
      "Epoch [57/100], Train Loss: 0.1042, Val Loss: 0.1065\n",
      "Epoch [58/100], Train Loss: 0.1018, Val Loss: 0.1021\n",
      "Epoch [59/100], Train Loss: 0.1004, Val Loss: 0.1042\n",
      "Epoch [60/100], Train Loss: 0.0987, Val Loss: 0.1036\n",
      "Epoch [61/100], Train Loss: 0.0963, Val Loss: 0.0967\n",
      "Epoch [62/100], Train Loss: 0.0955, Val Loss: 0.1010\n",
      "Epoch [63/100], Train Loss: 0.0927, Val Loss: 0.0927\n",
      "Epoch [64/100], Train Loss: 0.0904, Val Loss: 0.0927\n",
      "Epoch [65/100], Train Loss: 0.0880, Val Loss: 0.0907\n",
      "Epoch [66/100], Train Loss: 0.0862, Val Loss: 0.0881\n",
      "Epoch [67/100], Train Loss: 0.0843, Val Loss: 0.0871\n",
      "Epoch [68/100], Train Loss: 0.0828, Val Loss: 0.0852\n",
      "Epoch [69/100], Train Loss: 0.0808, Val Loss: 0.0831\n",
      "Epoch [70/100], Train Loss: 0.0787, Val Loss: 0.0810\n",
      "Epoch [71/100], Train Loss: 0.0771, Val Loss: 0.0785\n",
      "Epoch [72/100], Train Loss: 0.0755, Val Loss: 0.0784\n",
      "Epoch [73/100], Train Loss: 0.0739, Val Loss: 0.0772\n",
      "Epoch [74/100], Train Loss: 0.0723, Val Loss: 0.0755\n",
      "Epoch [75/100], Train Loss: 0.0709, Val Loss: 0.0737\n",
      "Epoch [76/100], Train Loss: 0.0696, Val Loss: 0.0740\n",
      "Epoch [77/100], Train Loss: 0.0681, Val Loss: 0.0720\n",
      "Epoch [78/100], Train Loss: 0.0667, Val Loss: 0.0708\n",
      "Epoch [79/100], Train Loss: 0.0653, Val Loss: 0.0695\n",
      "Epoch [80/100], Train Loss: 0.0640, Val Loss: 0.0680\n",
      "Epoch [81/100], Train Loss: 0.0627, Val Loss: 0.0677\n",
      "Epoch [82/100], Train Loss: 0.0615, Val Loss: 0.0664\n",
      "Epoch [83/100], Train Loss: 0.0602, Val Loss: 0.0650\n",
      "Epoch [84/100], Train Loss: 0.0591, Val Loss: 0.0637\n",
      "Epoch [85/100], Train Loss: 0.0582, Val Loss: 0.0618\n",
      "Epoch [86/100], Train Loss: 0.0571, Val Loss: 0.0615\n",
      "Epoch [87/100], Train Loss: 0.0559, Val Loss: 0.0612\n",
      "Epoch [88/100], Train Loss: 0.0548, Val Loss: 0.0585\n",
      "Epoch [89/100], Train Loss: 0.0540, Val Loss: 0.0569\n",
      "Epoch [90/100], Train Loss: 0.0531, Val Loss: 0.0572\n",
      "Epoch [91/100], Train Loss: 0.0522, Val Loss: 0.0563\n",
      "Epoch [92/100], Train Loss: 0.0510, Val Loss: 0.0550\n",
      "Epoch [93/100], Train Loss: 0.0503, Val Loss: 0.0545\n",
      "Epoch [94/100], Train Loss: 0.0491, Val Loss: 0.0561\n",
      "Epoch [95/100], Train Loss: 0.0484, Val Loss: 0.0531\n",
      "Epoch [96/100], Train Loss: 0.0479, Val Loss: 0.0532\n",
      "Epoch [97/100], Train Loss: 0.0470, Val Loss: 0.0518\n",
      "Epoch [98/100], Train Loss: 0.0461, Val Loss: 0.0493\n",
      "Epoch [99/100], Train Loss: 0.0451, Val Loss: 0.0501\n",
      "Epoch [100/100], Train Loss: 0.0441, Val Loss: 0.0485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenovo\\AppData\\Local\\Temp\\ipykernel_3032\\1826892759.py:311: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_model.pth'))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1. 自定义数据集类\n",
    "class FoveaDataset(Dataset):\n",
    "    def __init__(self, image_dir, xml_dir, fovea_csv=None, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.xml_dir = xml_dir\n",
    "        self.transform = transform\n",
    "        self.images = [f for f in os.listdir(image_dir) if f.endswith('.jpg')]\n",
    "        \n",
    "        # 加载中心凹坐标\n",
    "        if fovea_csv is not None:\n",
    "            self.fovea_coords = pd.read_csv(fovea_csv)\n",
    "        else:\n",
    "            self.fovea_coords = None\n",
    "    \n",
    "    def __len__(self):  # 添加这个方法\n",
    "        return len(self.images)\n",
    "            \n",
    "    def get_fovea_coords(self, img_name):\n",
    "        if self.fovea_coords is not None:\n",
    "            img_id = int(img_name.split('.')[0])\n",
    "            coords = self.fovea_coords[self.fovea_coords['data'] == img_id]\n",
    "            if not coords.empty:\n",
    "                return (coords['Fovea_X'].values[0], coords['Fovea_Y'].values[0])\n",
    "        return None\n",
    "\n",
    "    def create_fovea_heatmap(self, size, coords, original_size):\n",
    "        \"\"\"创建中心凹位置的高斯热图\"\"\"\n",
    "        x, y = coords\n",
    "        # 调整坐标到调整大小后的图像尺寸\n",
    "        x = int(x * size[0] / original_size[1])\n",
    "        y = int(y * size[1] / original_size[0])\n",
    "        \n",
    "        heatmap = np.zeros(size)\n",
    "        y = min(max(y, 0), size[1]-1)\n",
    "        x = min(max(x, 0), size[0]-1)\n",
    "        \n",
    "        # 创建高斯核\n",
    "        sigma = 5\n",
    "        kernel_size = 6 * sigma + 1\n",
    "        x_grid, y_grid = np.meshgrid(np.arange(size[0]), np.arange(size[1]))\n",
    "        heatmap = np.exp(-((x_grid - x)**2 + (y_grid - y)**2) / (2 * sigma**2))\n",
    "        heatmap = heatmap / heatmap.max()  # 归一化\n",
    "        \n",
    "        return heatmap\n",
    "\n",
    "    def parse_xml(self, xml_path):\n",
    "        tree = ET.parse(xml_path)\n",
    "        root = tree.getroot()\n",
    "        \n",
    "        size = root.find('size')\n",
    "        width = int(size.find('width').text)\n",
    "        height = int(size.find('height').text)\n",
    "        \n",
    "        obj = root.find('object')\n",
    "        bbox = obj.find('bndbox')\n",
    "        xmin = int(bbox.find('xmin').text)\n",
    "        ymin = int(bbox.find('ymin').text)\n",
    "        xmax = int(bbox.find('xmax').text)\n",
    "        ymax = int(bbox.find('ymax').text)\n",
    "        \n",
    "        return (width, height), (xmin, ymin, xmax, ymax)\n",
    "    \n",
    "    def create_mask(self, img_shape, bbox):\n",
    "        mask = np.zeros(img_shape[:2], dtype=np.float32)\n",
    "        xmin, ymin, xmax, ymax = bbox\n",
    "        mask[ymin:ymax, xmin:xmax] = 1\n",
    "        return mask\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.images[idx]\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "        xml_path = os.path.join(self.xml_dir, img_name.replace('.jpg', '.xml'))\n",
    "        \n",
    "        # 读取图像\n",
    "        image = cv2.imread(img_path)\n",
    "        orig_size = image.shape[:2]\n",
    "        \n",
    "        # 解析XML\n",
    "        _, bbox = self.parse_xml(xml_path)\n",
    "        mask = self.create_mask(orig_size, bbox)\n",
    "        \n",
    "        # 获取中心凹坐标并创建热图\n",
    "        coords = self.get_fovea_coords(img_name)\n",
    "        \n",
    "        # 调整大小\n",
    "        image = cv2.resize(image, (256, 256))\n",
    "        mask = cv2.resize(mask, (256, 256))\n",
    "        \n",
    "        # 转换为张量\n",
    "        image = image.transpose(2, 0, 1) / 255.0\n",
    "        image = torch.FloatTensor(image)\n",
    "        mask = torch.FloatTensor(mask).unsqueeze(0)\n",
    "        \n",
    "        if coords is not None:\n",
    "            heatmap = self.create_fovea_heatmap((256, 256), coords, orig_size)\n",
    "            heatmap = torch.FloatTensor(heatmap).unsqueeze(0)\n",
    "            return image, mask, heatmap, coords\n",
    "        \n",
    "        return image, mask, None, None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 2. 定义U-Net模型\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "    \n",
    "\n",
    "\n",
    "class UNetWithFovea(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 原有的U-Net编码器部分\n",
    "        self.enc1 = DoubleConv(3, 64)\n",
    "        self.enc2 = DoubleConv(64, 128)\n",
    "        self.enc3 = DoubleConv(128, 256)\n",
    "        self.enc4 = DoubleConv(256, 512)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        \n",
    "        # 原有的U-Net解码器部分\n",
    "        self.dec3 = DoubleConv(512 + 256, 256)\n",
    "        self.dec2 = DoubleConv(256 + 128, 128)\n",
    "        self.dec1 = DoubleConv(128 + 64, 64)\n",
    "        \n",
    "        # 分割分支\n",
    "        self.final_conv_seg = nn.Conv2d(64, 1, kernel_size=1)\n",
    "        \n",
    "        # 中心凹预测分支\n",
    "        self.final_conv_fovea = nn.Sequential(\n",
    "            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 1, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(self.pool(e1))\n",
    "        e3 = self.enc3(self.pool(e2))\n",
    "        e4 = self.enc4(self.pool(e3))\n",
    "        \n",
    "        # Decoder\n",
    "        d3 = self.dec3(torch.cat([self.upsample(e4), e3], dim=1))\n",
    "        d2 = self.dec2(torch.cat([self.upsample(d3), e2], dim=1))\n",
    "        d1 = self.dec1(torch.cat([self.upsample(d2), e1], dim=1))\n",
    "        \n",
    "        # 两个输出分支\n",
    "        mask = torch.sigmoid(self.final_conv_seg(d1))\n",
    "        fovea_heatmap = self.final_conv_fovea(d1)\n",
    "        \n",
    "        return mask, fovea_heatmap\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, device, num_epochs=100):\n",
    "    criterion_mask = nn.BCELoss()\n",
    "    criterion_fovea = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    for epoch in range(num_epochs):\n",
    "        # 训练阶段\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for images, masks, heatmaps, _ in train_loader:\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            if heatmaps is not None:\n",
    "                heatmaps = heatmaps.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            mask_pred, fovea_pred = model(images)\n",
    "            \n",
    "            loss_mask = criterion_mask(mask_pred, masks)\n",
    "            loss = loss_mask\n",
    "            \n",
    "            if heatmaps is not None:\n",
    "                loss_fovea = criterion_fovea(fovea_pred, heatmaps)\n",
    "                loss += loss_fovea\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        \n",
    "        # 验证阶段\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for images, masks, heatmaps, _ in val_loader:\n",
    "                images = images.to(device)\n",
    "                masks = masks.to(device)\n",
    "                if heatmaps is not None:\n",
    "                    heatmaps = heatmaps.to(device)\n",
    "                \n",
    "                mask_pred, fovea_pred = model(images)\n",
    "                loss = criterion_mask(mask_pred, masks)\n",
    "                \n",
    "                if heatmaps is not None:\n",
    "                    loss += criterion_fovea(fovea_pred, heatmaps)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "\n",
    "def predict_test_images(model, test_dir, device, output_csv):\n",
    "    model.eval()\n",
    "    os.makedirs('predictions', exist_ok=True)\n",
    "    \n",
    "    results = []\n",
    "    for img_name in sorted(os.listdir(test_dir)):\n",
    "        if img_name.endswith('.jpg'):\n",
    "            img_path = os.path.join(test_dir, img_name)\n",
    "            image = cv2.imread(img_path)\n",
    "            original_size = image.shape[:2]\n",
    "            \n",
    "            # 预处理\n",
    "            image_resized = cv2.resize(image, (256, 256))\n",
    "            image_tensor = torch.FloatTensor(image_resized.transpose(2, 0, 1) / 255.0).unsqueeze(0)\n",
    "            \n",
    "            # 预测\n",
    "            with torch.no_grad():\n",
    "                image_tensor = image_tensor.to(device)\n",
    "                _, fovea_heatmap = model(image_tensor)\n",
    "                fovea_heatmap = fovea_heatmap.cpu().numpy()[0, 0]\n",
    "            \n",
    "            # 找到热图中的最大值位置\n",
    "            y, x = np.unravel_index(np.argmax(fovea_heatmap), fovea_heatmap.shape)\n",
    "            \n",
    "            # 将坐标转换回原始图像大小\n",
    "            original_x = int(x * original_size[1] / 256)\n",
    "            original_y = int(y * original_size[0] / 256)\n",
    "            \n",
    "            # 保存结果\n",
    "            img_id = img_name.split('.')[0]\n",
    "            results.extend([\n",
    "                {'ImageID': f'{img_id}_Fovea_X', 'value': original_x},\n",
    "                {'ImageID': f'{img_id}_Fovea_Y', 'value': original_y}\n",
    "            ])\n",
    "            \n",
    "            # 可视化结果\n",
    "            plt.figure(figsize=(12, 4))\n",
    "            plt.subplot(131)\n",
    "            plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "            plt.plot(original_x, original_y, 'r+', markersize=10)\n",
    "            plt.title('Predicted Fovea Location')\n",
    "            plt.subplot(132)\n",
    "            plt.imshow(fovea_heatmap, cmap='jet')\n",
    "            plt.title('Fovea Heatmap')\n",
    "            plt.subplot(133)\n",
    "            plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "            plt.imshow(cv2.resize(fovea_heatmap, (original_size[1], original_size[0])), \n",
    "                      alpha=0.3, cmap='jet')\n",
    "            plt.plot(original_x, original_y, 'r+', markersize=10)\n",
    "            plt.title('Overlay')\n",
    "            plt.savefig(f'predictions/{img_name}_prediction.png')\n",
    "            plt.close()\n",
    "    \n",
    "    # 保存预测结果到CSV\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(output_csv, index=False)\n",
    "\n",
    "def main():\n",
    "    # 设置路径\n",
    "    train_image_dir = 'C:/code/vcpython/ML_design_1/task1/detection/train'\n",
    "    train_xml_dir = 'C:/code/vcpython/ML_design_1/task1/detection/train_location'\n",
    "    fovea_csv = 'C:/code/vcpython/ML_design_1/task1/detection/fovea_localization_train_GT.csv'  # 添加中心凹坐标文件路径\n",
    "    test_image_dir = 'C:/code/vcpython/ML_design_1/task1/detection/test'\n",
    "    output_csv = 'fovea_predictions.csv'\n",
    "    \n",
    "    # 创建数据集\n",
    "    dataset = FoveaDataset(train_image_dir, train_xml_dir, fovea_csv)\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "    \n",
    "    # 创建数据加载器\n",
    "    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=4)\n",
    "    \n",
    "    # 设置设备\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # 创建模型\n",
    "    model = UNetWithFovea().to(device)\n",
    "    \n",
    "    # 训练模型\n",
    "    train_model(model, train_loader, val_loader, device)\n",
    "    \n",
    "    # 加载最佳模型进行预测\n",
    "    model.load_state_dict(torch.load('best_model.pth'))\n",
    "    predict_test_images(model, test_image_dir, device, output_csv)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enc1.double_conv.0.weight torch.Size([64, 3, 3, 3])\n",
      "enc1.double_conv.0.bias torch.Size([64])\n",
      "enc1.double_conv.1.weight torch.Size([64])\n",
      "enc1.double_conv.1.bias torch.Size([64])\n",
      "enc1.double_conv.1.running_mean torch.Size([64])\n",
      "enc1.double_conv.1.running_var torch.Size([64])\n",
      "enc1.double_conv.1.num_batches_tracked torch.Size([])\n",
      "enc1.double_conv.3.weight torch.Size([64, 64, 3, 3])\n",
      "enc1.double_conv.3.bias torch.Size([64])\n",
      "enc1.double_conv.4.weight torch.Size([64])\n",
      "enc1.double_conv.4.bias torch.Size([64])\n",
      "enc1.double_conv.4.running_mean torch.Size([64])\n",
      "enc1.double_conv.4.running_var torch.Size([64])\n",
      "enc1.double_conv.4.num_batches_tracked torch.Size([])\n",
      "enc2.double_conv.0.weight torch.Size([128, 64, 3, 3])\n",
      "enc2.double_conv.0.bias torch.Size([128])\n",
      "enc2.double_conv.1.weight torch.Size([128])\n",
      "enc2.double_conv.1.bias torch.Size([128])\n",
      "enc2.double_conv.1.running_mean torch.Size([128])\n",
      "enc2.double_conv.1.running_var torch.Size([128])\n",
      "enc2.double_conv.1.num_batches_tracked torch.Size([])\n",
      "enc2.double_conv.3.weight torch.Size([128, 128, 3, 3])\n",
      "enc2.double_conv.3.bias torch.Size([128])\n",
      "enc2.double_conv.4.weight torch.Size([128])\n",
      "enc2.double_conv.4.bias torch.Size([128])\n",
      "enc2.double_conv.4.running_mean torch.Size([128])\n",
      "enc2.double_conv.4.running_var torch.Size([128])\n",
      "enc2.double_conv.4.num_batches_tracked torch.Size([])\n",
      "enc3.double_conv.0.weight torch.Size([256, 128, 3, 3])\n",
      "enc3.double_conv.0.bias torch.Size([256])\n",
      "enc3.double_conv.1.weight torch.Size([256])\n",
      "enc3.double_conv.1.bias torch.Size([256])\n",
      "enc3.double_conv.1.running_mean torch.Size([256])\n",
      "enc3.double_conv.1.running_var torch.Size([256])\n",
      "enc3.double_conv.1.num_batches_tracked torch.Size([])\n",
      "enc3.double_conv.3.weight torch.Size([256, 256, 3, 3])\n",
      "enc3.double_conv.3.bias torch.Size([256])\n",
      "enc3.double_conv.4.weight torch.Size([256])\n",
      "enc3.double_conv.4.bias torch.Size([256])\n",
      "enc3.double_conv.4.running_mean torch.Size([256])\n",
      "enc3.double_conv.4.running_var torch.Size([256])\n",
      "enc3.double_conv.4.num_batches_tracked torch.Size([])\n",
      "enc4.double_conv.0.weight torch.Size([512, 256, 3, 3])\n",
      "enc4.double_conv.0.bias torch.Size([512])\n",
      "enc4.double_conv.1.weight torch.Size([512])\n",
      "enc4.double_conv.1.bias torch.Size([512])\n",
      "enc4.double_conv.1.running_mean torch.Size([512])\n",
      "enc4.double_conv.1.running_var torch.Size([512])\n",
      "enc4.double_conv.1.num_batches_tracked torch.Size([])\n",
      "enc4.double_conv.3.weight torch.Size([512, 512, 3, 3])\n",
      "enc4.double_conv.3.bias torch.Size([512])\n",
      "enc4.double_conv.4.weight torch.Size([512])\n",
      "enc4.double_conv.4.bias torch.Size([512])\n",
      "enc4.double_conv.4.running_mean torch.Size([512])\n",
      "enc4.double_conv.4.running_var torch.Size([512])\n",
      "enc4.double_conv.4.num_batches_tracked torch.Size([])\n",
      "dec3.double_conv.0.weight torch.Size([256, 768, 3, 3])\n",
      "dec3.double_conv.0.bias torch.Size([256])\n",
      "dec3.double_conv.1.weight torch.Size([256])\n",
      "dec3.double_conv.1.bias torch.Size([256])\n",
      "dec3.double_conv.1.running_mean torch.Size([256])\n",
      "dec3.double_conv.1.running_var torch.Size([256])\n",
      "dec3.double_conv.1.num_batches_tracked torch.Size([])\n",
      "dec3.double_conv.3.weight torch.Size([256, 256, 3, 3])\n",
      "dec3.double_conv.3.bias torch.Size([256])\n",
      "dec3.double_conv.4.weight torch.Size([256])\n",
      "dec3.double_conv.4.bias torch.Size([256])\n",
      "dec3.double_conv.4.running_mean torch.Size([256])\n",
      "dec3.double_conv.4.running_var torch.Size([256])\n",
      "dec3.double_conv.4.num_batches_tracked torch.Size([])\n",
      "dec2.double_conv.0.weight torch.Size([128, 384, 3, 3])\n",
      "dec2.double_conv.0.bias torch.Size([128])\n",
      "dec2.double_conv.1.weight torch.Size([128])\n",
      "dec2.double_conv.1.bias torch.Size([128])\n",
      "dec2.double_conv.1.running_mean torch.Size([128])\n",
      "dec2.double_conv.1.running_var torch.Size([128])\n",
      "dec2.double_conv.1.num_batches_tracked torch.Size([])\n",
      "dec2.double_conv.3.weight torch.Size([128, 128, 3, 3])\n",
      "dec2.double_conv.3.bias torch.Size([128])\n",
      "dec2.double_conv.4.weight torch.Size([128])\n",
      "dec2.double_conv.4.bias torch.Size([128])\n",
      "dec2.double_conv.4.running_mean torch.Size([128])\n",
      "dec2.double_conv.4.running_var torch.Size([128])\n",
      "dec2.double_conv.4.num_batches_tracked torch.Size([])\n",
      "dec1.double_conv.0.weight torch.Size([64, 192, 3, 3])\n",
      "dec1.double_conv.0.bias torch.Size([64])\n",
      "dec1.double_conv.1.weight torch.Size([64])\n",
      "dec1.double_conv.1.bias torch.Size([64])\n",
      "dec1.double_conv.1.running_mean torch.Size([64])\n",
      "dec1.double_conv.1.running_var torch.Size([64])\n",
      "dec1.double_conv.1.num_batches_tracked torch.Size([])\n",
      "dec1.double_conv.3.weight torch.Size([64, 64, 3, 3])\n",
      "dec1.double_conv.3.bias torch.Size([64])\n",
      "dec1.double_conv.4.weight torch.Size([64])\n",
      "dec1.double_conv.4.bias torch.Size([64])\n",
      "dec1.double_conv.4.running_mean torch.Size([64])\n",
      "dec1.double_conv.4.running_var torch.Size([64])\n",
      "dec1.double_conv.4.num_batches_tracked torch.Size([])\n",
      "final_conv_seg.weight torch.Size([1, 64, 1, 1])\n",
      "final_conv_seg.bias torch.Size([1])\n",
      "final_conv_fovea.0.weight torch.Size([32, 64, 3, 3])\n",
      "final_conv_fovea.0.bias torch.Size([32])\n",
      "final_conv_fovea.2.weight torch.Size([1, 32, 1, 1])\n",
      "final_conv_fovea.2.bias torch.Size([1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenovo\\AppData\\Local\\Temp\\ipykernel_11792\\115303425.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('best_model.pth')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "checkpoint = torch.load('best_model.pth')\n",
    "for name, param in checkpoint.items():\n",
    "    print(name, param.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
