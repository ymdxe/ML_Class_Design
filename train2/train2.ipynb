{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import pandas as pd \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import albumentations as A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unet训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用设备: cuda\n",
      "GPU名称: NVIDIA GeForce RTX 3050 Laptop GPU\n",
      "GPU显存总量: 4096MB\n",
      "预处理图像...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [00:05<00:00, 15.36it/s]\n",
      "C:\\Users\\lenovo\\AppData\\Local\\Temp\\ipykernel_13940\\1784769597.py:503: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_model.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "begin training\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1. 自定义数据集类\n",
    "from torch import GradScaler\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class FoveaDataset(Dataset):\n",
    "    def __init__(self, image_dir, xml_dir, fovea_csv=None, transform=None, is_test=False):\n",
    "        self.image_dir = image_dir\n",
    "        self.xml_dir = xml_dir\n",
    "        self.transform = A.Compose([  # 使用albumentations\n",
    "            A.CLAHE(clip_limit=3.0, tile_grid_size=(8, 8)),\n",
    "            A.Sharpen(alpha=(0.2, 0.5), lightness=(0.5, 1.0)),\n",
    "        ])\n",
    "        # self.transform = transform\n",
    "        self.is_test = is_test\n",
    "        self.images = [f for f in os.listdir(image_dir) if f.endswith('.jpg')]\n",
    "        if fovea_csv is not None:\n",
    "            self.fovea_coords = pd.read_csv(fovea_csv)\n",
    "        else:\n",
    "            self.fovea_coords = None\n",
    "         \n",
    "        # 预处理所有图像\n",
    "        self.processed_images = {}\n",
    "        print(\"预处理图像...\")\n",
    "        for img_name in tqdm(self.images):\n",
    "            img_path = os.path.join(image_dir, img_name)\n",
    "            image = cv2.imread(img_path)\n",
    "            enhanced_image = self.enhance_image(image)\n",
    "            self.processed_images[img_name] = enhanced_image\n",
    "            \n",
    "    \n",
    "    def enhance_image(self, image):\n",
    "        \"\"\"简化的图像增强处理\"\"\"\n",
    "        lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
    "        l, a, b = cv2.split(lab)\n",
    "        clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))\n",
    "        l = clahe.apply(l)\n",
    "        enhanced_lab = cv2.merge((l,a,b))\n",
    "        enhanced_image = cv2.cvtColor(enhanced_lab, cv2.COLOR_LAB2BGR)\n",
    "        \n",
    "        return enhanced_image\n",
    "    \n",
    "    def __len__(self):  # 添加这个方法\n",
    "        return len(self.images)\n",
    "            \n",
    "    def get_fovea_coords(self, img_name):\n",
    "        if self.fovea_coords is not None:\n",
    "            img_id = int(img_name.split('.')[0])\n",
    "            coords = self.fovea_coords[self.fovea_coords['data'] == img_id]\n",
    "            if not coords.empty:\n",
    "                return (coords['Fovea_X'].values[0], coords['Fovea_Y'].values[0])\n",
    "        return None\n",
    "\n",
    "    def create_fovea_heatmap(self, size, coords, original_size):\n",
    "        \"\"\"创建中心凹位置的高斯热图\"\"\"\n",
    "        x, y = coords\n",
    "        # 调整坐标到调整大小后的图像尺寸\n",
    "        x = int(x * size[0] / original_size[1])\n",
    "        y = int(y * size[1] / original_size[0])\n",
    "        \n",
    "        heatmap = np.zeros(size)\n",
    "        y = min(max(y, 0), size[1]-1)\n",
    "        x = min(max(x, 0), size[0]-1)\n",
    "        \n",
    "        # 创建高斯核\n",
    "        sigma = 5\n",
    "        kernel_size = 6 * sigma + 1\n",
    "        x_grid, y_grid = np.meshgrid(np.arange(size[0]), np.arange(size[1]))\n",
    "        heatmap = np.exp(-((x_grid - x)**2 + (y_grid - y)**2) / (2 * sigma**2))\n",
    "        heatmap = heatmap / heatmap.max()  # 归一化\n",
    "        \n",
    "        return heatmap\n",
    "\n",
    "    def parse_xml(self, xml_path):\n",
    "        tree = ET.parse(xml_path)\n",
    "        root = tree.getroot()\n",
    "        \n",
    "        size = root.find('size')\n",
    "        width = int(size.find('width').text)\n",
    "        height = int(size.find('height').text)\n",
    "        \n",
    "        obj = root.find('object')\n",
    "        bbox = obj.find('bndbox')\n",
    "        xmin = int(bbox.find('xmin').text)\n",
    "        ymin = int(bbox.find('ymin').text)\n",
    "        xmax = int(bbox.find('xmax').text)\n",
    "        ymax = int(bbox.find('ymax').text)\n",
    "        \n",
    "        return (width, height), (xmin, ymin, xmax, ymax)\n",
    "    \n",
    "    def create_mask(self, img_shape, bbox):\n",
    "        mask = np.zeros(img_shape[:2], dtype=np.float32)\n",
    "        xmin, ymin, xmax, ymax = bbox\n",
    "        mask[ymin:ymax, xmin:xmax] = 1\n",
    "        return mask\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.images[idx]\n",
    "        # 直接使用预处理好的图像\n",
    "        image = self.processed_images[img_name]\n",
    "        \n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "        xml_path = os.path.join(self.xml_dir, img_name.replace('.jpg', '.xml'))\n",
    "        \n",
    "        # 读取图像\n",
    "        image = cv2.imread(img_path)\n",
    "        \n",
    "        image = self.enhance_image(image)  # 添加图像增强\n",
    "        \n",
    "        orig_size = image.shape[:2]\n",
    "        \n",
    "        # 解析XML\n",
    "        _, bbox = self.parse_xml(xml_path)\n",
    "        mask = self.create_mask(orig_size, bbox)\n",
    "        \n",
    "        # 获取中心凹坐标并创建热图\n",
    "        coords = self.get_fovea_coords(img_name)\n",
    "        \n",
    "        # 调整大小\n",
    "        image = cv2.resize(image, (256, 256))\n",
    "        mask = cv2.resize(mask, (256, 256))\n",
    "        \n",
    "        # 转换为张量\n",
    "        image = image.transpose(2, 0, 1) / 255.0\n",
    "        image = torch.FloatTensor(image)\n",
    "        mask = torch.FloatTensor(mask).unsqueeze(0)\n",
    "        \n",
    "        if coords is not None:\n",
    "            heatmap = self.create_fovea_heatmap((256, 256), coords, orig_size)\n",
    "            heatmap = torch.FloatTensor(heatmap).unsqueeze(0)\n",
    "            return image, mask, heatmap, coords\n",
    "        \n",
    "        return image, mask, None, None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 2. 定义U-Net模型\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "    \n",
    "\n",
    "\n",
    "class UNetWithFovea(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 原有的U-Net编码器部分\n",
    "        self.enc1 = DoubleConv(3, 64)\n",
    "        self.enc2 = DoubleConv(64, 128)\n",
    "        self.enc3 = DoubleConv(128, 256)\n",
    "        self.enc4 = DoubleConv(256, 512)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        \n",
    "        # 原有的U-Net解码器部分\n",
    "        self.dec3 = DoubleConv(512 + 256, 256)\n",
    "        self.dec2 = DoubleConv(256 + 128, 128)\n",
    "        self.dec1 = DoubleConv(128 + 64, 64)\n",
    "        \n",
    "        # 分割分支\n",
    "        self.final_conv_seg = nn.Conv2d(64, 1, kernel_size=1)\n",
    "        \n",
    "        # 中心凹预测分支\n",
    "        self.final_conv_fovea = nn.Sequential(\n",
    "            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 1, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(self.pool(e1))\n",
    "        e3 = self.enc3(self.pool(e2))\n",
    "        e4 = self.enc4(self.pool(e3))\n",
    "        \n",
    "        # Decoder\n",
    "        d3 = self.dec3(torch.cat([self.upsample(e4), e3], dim=1))\n",
    "        d2 = self.dec2(torch.cat([self.upsample(d3), e2], dim=1))\n",
    "        d1 = self.dec1(torch.cat([self.upsample(d2), e1], dim=1))\n",
    "        \n",
    "        # 两个输出分支\n",
    "        mask = torch.sigmoid(self.final_conv_seg(d1))\n",
    "        fovea_heatmap = self.final_conv_fovea(d1)\n",
    "        \n",
    "        return mask, fovea_heatmap\n",
    "\n",
    "def train_model(model, train_loader, val_loader, device, num_epochs=100):\n",
    "    criterion_mask = nn.BCELoss()\n",
    "    criterion_fovea = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    for epoch in range(num_epochs):\n",
    "        # 训练阶段\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for images, masks, heatmaps, _ in train_loader:\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            if heatmaps is not None:\n",
    "                heatmaps = heatmaps.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            mask_pred, fovea_pred = model(images)\n",
    "            \n",
    "            loss_mask = criterion_mask(mask_pred, masks)\n",
    "            loss = loss_mask\n",
    "            \n",
    "            if heatmaps is not None:\n",
    "                loss_fovea = criterion_fovea(fovea_pred, heatmaps)\n",
    "                loss += loss_fovea\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        \n",
    "        # 验证阶段\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for images, masks, heatmaps, _ in val_loader:\n",
    "                images = images.to(device)\n",
    "                masks = masks.to(device)\n",
    "                if heatmaps is not None:\n",
    "                    heatmaps = heatmaps.to(device)\n",
    "                \n",
    "                mask_pred, fovea_pred = model(images)\n",
    "                loss = criterion_mask(mask_pred, masks)\n",
    "                \n",
    "                if heatmaps is not None:\n",
    "                    loss += criterion_fovea(fovea_pred, heatmaps)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "\n",
    "def pad_image(image, target_size):\n",
    "    \"\"\"填充图像到目标大小\"\"\"\n",
    "    h, w = image.shape[:2]\n",
    "    target_h, target_w = target_size\n",
    "    \n",
    "    # 计算需要的填充\n",
    "    pad_h = max(0, target_h - h)\n",
    "    pad_w = max(0, target_w - w)\n",
    "    \n",
    "    # 计算上下左右填充量\n",
    "    top = pad_h // 2\n",
    "    bottom = pad_h - top\n",
    "    left = pad_w // 2\n",
    "    right = pad_w - left\n",
    "    \n",
    "    # 填充图像\n",
    "    padded_image = cv2.copyMakeBorder(image, top, bottom, left, right,\n",
    "                                     cv2.BORDER_CONSTANT, value=(0,0,0))\n",
    "    return padded_image\n",
    "\n",
    "def predict_test_images(model, test_dir, device, output_csv):\n",
    "    model.eval()\n",
    "    os.makedirs('predictions', exist_ok=True)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for img_name in sorted(os.listdir(test_dir)):\n",
    "        if img_name.endswith('.jpg'):\n",
    "            img_path = os.path.join(test_dir, img_name)\n",
    "            image = cv2.imread(img_path)\n",
    "            original_size = image.shape[:2]\n",
    "            \n",
    "            # 图像增强\n",
    "            enhanced_image = enhance_image(image)\n",
    "            \n",
    "            # 多尺度预测\n",
    "            scales = [0.75, 1.0, 1.25]\n",
    "            fovea_heatmaps = []\n",
    "            \n",
    "            for scale in scales:\n",
    "                # 计算缩放尺寸\n",
    "                scaled_size = (int(256 * scale), int(256 * scale))\n",
    "                image_scaled = cv2.resize(enhanced_image, scaled_size)\n",
    "                \n",
    "                # 安全地填充到256x256\n",
    "                if scale != 1.0:\n",
    "                    image_scaled = pad_image(image_scaled, (256, 256))\n",
    "                \n",
    "                # 确保图像大小正确\n",
    "                image_scaled = cv2.resize(image_scaled, (256, 256))\n",
    "                \n",
    "                # 转换为张量\n",
    "                image_tensor = torch.FloatTensor(image_scaled.transpose(2, 0, 1) / 255.0).unsqueeze(0)\n",
    "                \n",
    "                # 预测\n",
    "                with torch.no_grad():\n",
    "                    image_tensor = image_tensor.to(device)\n",
    "                    _, fovea_heatmap = model(image_tensor)\n",
    "                    fovea_heatmap = fovea_heatmap.cpu().numpy()[0, 0]\n",
    "                    fovea_heatmaps.append(fovea_heatmap)\n",
    "            \n",
    "            # 融合多尺度预测结果\n",
    "            fovea_heatmap = np.mean(fovea_heatmaps, axis=0)\n",
    "            \n",
    "            # 使用高斯滤波平滑热图\n",
    "            fovea_heatmap = cv2.GaussianBlur(fovea_heatmap, (7, 7), 0)\n",
    "            \n",
    "            # 找到热图中的最大值位置\n",
    "            y, x = np.unravel_index(np.argmax(fovea_heatmap), fovea_heatmap.shape)\n",
    "            \n",
    "            # 将坐标转换回原始图像大小\n",
    "            original_x = int(x * original_size[1] / 256)\n",
    "            original_y = int(y * original_size[0] / 256)\n",
    "            \n",
    "            # 验证预测结果\n",
    "            if validate_prediction(original_x, original_y, original_size):\n",
    "                results.extend([\n",
    "                    {'ImageID': f'{img_name.split(\".\")[0]}_Fovea_X', 'value': original_x},\n",
    "                    {'ImageID': f'{img_name.split(\".\")[0]}_Fovea_Y', 'value': original_y}\n",
    "                ])\n",
    "            else:\n",
    "                backup_x, backup_y = get_backup_prediction(image)\n",
    "                results.extend([\n",
    "                    {'ImageID': f'{img_name.split(\".\")[0]}_Fovea_X', 'value': backup_x},\n",
    "                    {'ImageID': f'{img_name.split(\".\")[0]}_Fovea_Y', 'value': backup_y}\n",
    "                ])\n",
    "            \n",
    "            # 可视化结果\n",
    "            plt.figure(figsize=(12, 4))\n",
    "            plt.subplot(131)\n",
    "            plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "            plt.plot(original_x, original_y, 'r+', markersize=10)\n",
    "            plt.title('Predicted Fovea Location')\n",
    "            \n",
    "            plt.subplot(132)\n",
    "            plt.imshow(fovea_heatmap, cmap='jet')\n",
    "            plt.title('Fovea Heatmap')\n",
    "            \n",
    "            plt.subplot(133)\n",
    "            plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "            plt.imshow(cv2.resize(fovea_heatmap, (original_size[1], original_size[0])), \n",
    "                      alpha=0.3, cmap='jet')\n",
    "            plt.plot(original_x, original_y, 'r+', markersize=10)\n",
    "            plt.title('Overlay')\n",
    "            \n",
    "            plt.savefig(f'predictions/{img_name}_prediction.png')\n",
    "            plt.close()\n",
    "    \n",
    "    # 保存预测结果到CSV\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(output_csv, index=False)\n",
    "\n",
    "\n",
    "def enhance_image(image):\n",
    "    \"\"\"图像增强处理\"\"\"\n",
    "    # 对比度增强\n",
    "    lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
    "    l, a, b = cv2.split(lab)\n",
    "    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))\n",
    "    l = clahe.apply(l)\n",
    "    enhanced_lab = cv2.merge((l,a,b))\n",
    "    enhanced_image = cv2.cvtColor(enhanced_lab, cv2.COLOR_LAB2BGR)\n",
    "    \n",
    "    # 锐化\n",
    "    kernel = np.array([[-1,-1,-1],\n",
    "                      [-1, 9,-1],\n",
    "                      [-1,-1,-1]])\n",
    "    sharpened = cv2.filter2D(enhanced_image, -1, kernel)\n",
    "    \n",
    "    # 去噪\n",
    "    denoised = cv2.fastNlMeansDenoisingColored(sharpened)\n",
    "    \n",
    "    return denoised\n",
    "\n",
    "def validate_prediction(x, y, image_size):\n",
    "    \"\"\"验证预测的坐标是否在合理范围内\"\"\"\n",
    "    # 检查坐标是否在图像范围内\n",
    "    if x < 0 or x >= image_size[1] or y < 0 or y >= image_size[0]:\n",
    "        return False\n",
    "    \n",
    "    # 检查坐标是否在图像中心区域附近\n",
    "    center_x = image_size[1] // 2\n",
    "    center_y = image_size[0] // 2\n",
    "    max_distance = min(image_size) * 0.4  # 假设中心凹不会距离中心太远\n",
    "    \n",
    "    distance = np.sqrt((x - center_x)**2 + (y - center_y)**2)\n",
    "    return distance <= max_distance\n",
    "\n",
    "def get_backup_prediction(image):\n",
    "    \"\"\"当主要预测失败时的备选预测策略\"\"\"\n",
    "    # 这里可以实现一个基于传统图像处理的备选方法\n",
    "    # 例如：使用图像的中心点或基于解剖学特征的启发式方法\n",
    "    \n",
    "    # 简单示例：返回图像中心点\n",
    "    h, w = image.shape[:2]\n",
    "    return w//2, h//2\n",
    "\n",
    "def visualize_prediction(original_image, enhanced_image, heatmap, pred_x, pred_y, img_name):\n",
    "    \"\"\"可视化预测结果\"\"\"\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # 原始图像\n",
    "    plt.subplot(141)\n",
    "    plt.imshow(cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB))\n",
    "    plt.plot(pred_x, pred_y, 'r+', markersize=10)\n",
    "    plt.title('Original Image')\n",
    "    \n",
    "    # 增强后的图像\n",
    "    plt.subplot(142)\n",
    "    plt.imshow(cv2.cvtColor(enhanced_image, cv2.COLOR_BGR2RGB))\n",
    "    plt.plot(pred_x, pred_y, 'r+', markersize=10)\n",
    "    plt.title('Enhanced Image')\n",
    "    \n",
    "    # 热图\n",
    "    plt.subplot(143)\n",
    "    plt.imshow(heatmap, cmap='jet')\n",
    "    plt.title('Fovea Heatmap')\n",
    "    \n",
    "    # 叠加显示\n",
    "    plt.subplot(144)\n",
    "    plt.imshow(cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB))\n",
    "    plt.imshow(cv2.resize(heatmap, (original_image.shape[1], original_image.shape[0])), \n",
    "              alpha=0.3, cmap='jet')\n",
    "    plt.plot(pred_x, pred_y, 'r+', markersize=10)\n",
    "    plt.title('Overlay')\n",
    "    \n",
    "    plt.savefig(f'predictions/{img_name}_prediction.png')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def main():\n",
    "    # 设置路径\n",
    "    train_image_dir = 'C:/code/vcpython/ML_design_1/task1/detection/train'\n",
    "    train_xml_dir = 'C:/code/vcpython/ML_design_1/task1/detection/train_location'\n",
    "    fovea_csv = 'C:/code/vcpython/ML_design_1/task1/detection/fovea_localization_train_GT.csv'  # 添加中心凹坐标文件路径\n",
    "    test_image_dir = 'C:/code/vcpython/ML_design_1/task1/detection/test'\n",
    "    output_csv = 'fovea_predictions.csv'\n",
    "    \n",
    "    torch.backends.cudnn.benchmark = True  # 加速卷积运算\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True  # 允许使用TF32\n",
    "    torch.backends.cudnn.allow_tf32 = True  # 允许cudnn使用TF32\n",
    "\n",
    "    # 检查CUDA是否可用\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f'使用设备: {device}')\n",
    "    if torch.cuda.is_available():\n",
    "        print(f'GPU名称: {torch.cuda.get_device_name(0)}')\n",
    "        print(f'GPU显存总量: {torch.cuda.get_device_properties(0).total_memory / 1024 ** 2:.0f}MB')\n",
    "    \n",
    "    # 创建数据集\n",
    "    dataset = FoveaDataset(train_image_dir, train_xml_dir, fovea_csv)\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "    \n",
    "    # 创建数据加载器\n",
    "    batch_size = 16  # 或者更大，具体取决于你的GPU内存\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=batch_size,\n",
    "        num_workers=0,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # 设置设备\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    print(torch.cuda.is_available())\n",
    "    device = torch.device('cuda:0')\n",
    "    # 创建模型\n",
    "    model = UNetWithFovea().to(device)\n",
    "    \n",
    "    print(\"begin training\")\n",
    "    # 训练模型\n",
    "    # train_model(model, train_loader, val_loader, device)\n",
    "    \n",
    "    # 加载最佳模型进行预测\n",
    "    model.load_state_dict(torch.load('best_model.pth'))\n",
    "    predict_test_images(model, test_image_dir, device, output_csv)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA是否可用: True\n",
      "当前CUDA设备号: 0\n",
      "CUDA设备名称: NVIDIA GeForce RTX 3050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"CUDA是否可用:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"当前CUDA设备号:\", torch.cuda.current_device())\n",
    "    print(\"CUDA设备名称:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "print(os.environ.get('CUDA_VISIBLE_DEVICES'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enc1.double_conv.0.weight torch.Size([64, 3, 3, 3])\n",
      "enc1.double_conv.0.bias torch.Size([64])\n",
      "enc1.double_conv.1.weight torch.Size([64])\n",
      "enc1.double_conv.1.bias torch.Size([64])\n",
      "enc1.double_conv.1.running_mean torch.Size([64])\n",
      "enc1.double_conv.1.running_var torch.Size([64])\n",
      "enc1.double_conv.1.num_batches_tracked torch.Size([])\n",
      "enc1.double_conv.3.weight torch.Size([64, 64, 3, 3])\n",
      "enc1.double_conv.3.bias torch.Size([64])\n",
      "enc1.double_conv.4.weight torch.Size([64])\n",
      "enc1.double_conv.4.bias torch.Size([64])\n",
      "enc1.double_conv.4.running_mean torch.Size([64])\n",
      "enc1.double_conv.4.running_var torch.Size([64])\n",
      "enc1.double_conv.4.num_batches_tracked torch.Size([])\n",
      "enc2.double_conv.0.weight torch.Size([128, 64, 3, 3])\n",
      "enc2.double_conv.0.bias torch.Size([128])\n",
      "enc2.double_conv.1.weight torch.Size([128])\n",
      "enc2.double_conv.1.bias torch.Size([128])\n",
      "enc2.double_conv.1.running_mean torch.Size([128])\n",
      "enc2.double_conv.1.running_var torch.Size([128])\n",
      "enc2.double_conv.1.num_batches_tracked torch.Size([])\n",
      "enc2.double_conv.3.weight torch.Size([128, 128, 3, 3])\n",
      "enc2.double_conv.3.bias torch.Size([128])\n",
      "enc2.double_conv.4.weight torch.Size([128])\n",
      "enc2.double_conv.4.bias torch.Size([128])\n",
      "enc2.double_conv.4.running_mean torch.Size([128])\n",
      "enc2.double_conv.4.running_var torch.Size([128])\n",
      "enc2.double_conv.4.num_batches_tracked torch.Size([])\n",
      "enc3.double_conv.0.weight torch.Size([256, 128, 3, 3])\n",
      "enc3.double_conv.0.bias torch.Size([256])\n",
      "enc3.double_conv.1.weight torch.Size([256])\n",
      "enc3.double_conv.1.bias torch.Size([256])\n",
      "enc3.double_conv.1.running_mean torch.Size([256])\n",
      "enc3.double_conv.1.running_var torch.Size([256])\n",
      "enc3.double_conv.1.num_batches_tracked torch.Size([])\n",
      "enc3.double_conv.3.weight torch.Size([256, 256, 3, 3])\n",
      "enc3.double_conv.3.bias torch.Size([256])\n",
      "enc3.double_conv.4.weight torch.Size([256])\n",
      "enc3.double_conv.4.bias torch.Size([256])\n",
      "enc3.double_conv.4.running_mean torch.Size([256])\n",
      "enc3.double_conv.4.running_var torch.Size([256])\n",
      "enc3.double_conv.4.num_batches_tracked torch.Size([])\n",
      "enc4.double_conv.0.weight torch.Size([512, 256, 3, 3])\n",
      "enc4.double_conv.0.bias torch.Size([512])\n",
      "enc4.double_conv.1.weight torch.Size([512])\n",
      "enc4.double_conv.1.bias torch.Size([512])\n",
      "enc4.double_conv.1.running_mean torch.Size([512])\n",
      "enc4.double_conv.1.running_var torch.Size([512])\n",
      "enc4.double_conv.1.num_batches_tracked torch.Size([])\n",
      "enc4.double_conv.3.weight torch.Size([512, 512, 3, 3])\n",
      "enc4.double_conv.3.bias torch.Size([512])\n",
      "enc4.double_conv.4.weight torch.Size([512])\n",
      "enc4.double_conv.4.bias torch.Size([512])\n",
      "enc4.double_conv.4.running_mean torch.Size([512])\n",
      "enc4.double_conv.4.running_var torch.Size([512])\n",
      "enc4.double_conv.4.num_batches_tracked torch.Size([])\n",
      "dec3.double_conv.0.weight torch.Size([256, 768, 3, 3])\n",
      "dec3.double_conv.0.bias torch.Size([256])\n",
      "dec3.double_conv.1.weight torch.Size([256])\n",
      "dec3.double_conv.1.bias torch.Size([256])\n",
      "dec3.double_conv.1.running_mean torch.Size([256])\n",
      "dec3.double_conv.1.running_var torch.Size([256])\n",
      "dec3.double_conv.1.num_batches_tracked torch.Size([])\n",
      "dec3.double_conv.3.weight torch.Size([256, 256, 3, 3])\n",
      "dec3.double_conv.3.bias torch.Size([256])\n",
      "dec3.double_conv.4.weight torch.Size([256])\n",
      "dec3.double_conv.4.bias torch.Size([256])\n",
      "dec3.double_conv.4.running_mean torch.Size([256])\n",
      "dec3.double_conv.4.running_var torch.Size([256])\n",
      "dec3.double_conv.4.num_batches_tracked torch.Size([])\n",
      "dec2.double_conv.0.weight torch.Size([128, 384, 3, 3])\n",
      "dec2.double_conv.0.bias torch.Size([128])\n",
      "dec2.double_conv.1.weight torch.Size([128])\n",
      "dec2.double_conv.1.bias torch.Size([128])\n",
      "dec2.double_conv.1.running_mean torch.Size([128])\n",
      "dec2.double_conv.1.running_var torch.Size([128])\n",
      "dec2.double_conv.1.num_batches_tracked torch.Size([])\n",
      "dec2.double_conv.3.weight torch.Size([128, 128, 3, 3])\n",
      "dec2.double_conv.3.bias torch.Size([128])\n",
      "dec2.double_conv.4.weight torch.Size([128])\n",
      "dec2.double_conv.4.bias torch.Size([128])\n",
      "dec2.double_conv.4.running_mean torch.Size([128])\n",
      "dec2.double_conv.4.running_var torch.Size([128])\n",
      "dec2.double_conv.4.num_batches_tracked torch.Size([])\n",
      "dec1.double_conv.0.weight torch.Size([64, 192, 3, 3])\n",
      "dec1.double_conv.0.bias torch.Size([64])\n",
      "dec1.double_conv.1.weight torch.Size([64])\n",
      "dec1.double_conv.1.bias torch.Size([64])\n",
      "dec1.double_conv.1.running_mean torch.Size([64])\n",
      "dec1.double_conv.1.running_var torch.Size([64])\n",
      "dec1.double_conv.1.num_batches_tracked torch.Size([])\n",
      "dec1.double_conv.3.weight torch.Size([64, 64, 3, 3])\n",
      "dec1.double_conv.3.bias torch.Size([64])\n",
      "dec1.double_conv.4.weight torch.Size([64])\n",
      "dec1.double_conv.4.bias torch.Size([64])\n",
      "dec1.double_conv.4.running_mean torch.Size([64])\n",
      "dec1.double_conv.4.running_var torch.Size([64])\n",
      "dec1.double_conv.4.num_batches_tracked torch.Size([])\n",
      "final_conv_seg.weight torch.Size([1, 64, 1, 1])\n",
      "final_conv_seg.bias torch.Size([1])\n",
      "final_conv_fovea.0.weight torch.Size([32, 64, 3, 3])\n",
      "final_conv_fovea.0.bias torch.Size([32])\n",
      "final_conv_fovea.2.weight torch.Size([1, 32, 1, 1])\n",
      "final_conv_fovea.2.bias torch.Size([1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenovo\\AppData\\Local\\Temp\\ipykernel_11792\\115303425.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('best_model.pth')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "checkpoint = torch.load('best_model.pth')\n",
    "for name, param in checkpoint.items():\n",
    "    print(name, param.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
